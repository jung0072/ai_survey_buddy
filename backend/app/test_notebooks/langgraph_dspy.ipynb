{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangGraph + DSPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the dependencies if needed.\n",
    "# %pip install -U dspy-ai\n",
    "# %pip install -U openai jinja2\n",
    "# %pip install -U langchain langchain-community langchain-openai langchain-core langgraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0) source code modification\n",
    "\n",
    "```\n",
    "class LangChainPredict(Predict, Runnable):\n",
    "    def __init__(self, prompt, llm, state_graph=False, **config):\n",
    "        self.state_g = state_graph\n",
    "\n",
    "\n",
    "    def forward():\n",
    "        if self.state_graph:\n",
    "            return {\"langpredict_output\": output}   \n",
    "```\n",
    "\n",
    "state_graph parameter is added to return a dictionary to update the state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Setting Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/minkijung/anaconda3/envs/survey_buddy/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import dspy\n",
    "\n",
    "from dspy.evaluate.evaluate import Evaluate\n",
    "from dspy.teleprompt import BootstrapFewShotWithRandomSearch\n",
    "\n",
    "colbertv2 = dspy.ColBERTv2(url='http://20.102.90.50:2017/wiki17_abstracts')\n",
    "\n",
    "dspy.configure(rm=colbertv2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI\n",
    "from langchain.cache import SQLiteCache\n",
    "\n",
    "llm = OpenAI(model_name=\"gpt-3.5-turbo-instruct\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def retrieve(state):\n",
    "#     print(\"==> retrieve\")\n",
    "#     result = dspy.Retrieve(k=5)(state[\"question\"]).passages\n",
    "#     print(\"result\", result)\n",
    "#     return {\"context\": result}\n",
    "retrieve = lambda x: dspy.Retrieve(k=5)(x[\"question\"]).passages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Defining a graph as a `LangGraph` expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "from dspy.predict.langchain import LangChainPredict\n",
    "\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"Given {context}, answer the question `{question}` as a tweet.\"\n",
    ")\n",
    "\n",
    "# ! LangChainPredict not working with StateGraph since it has to return dict\n",
    "# Modified DSPy langchain.py to make it work with StateGraphs\n",
    "# use state_graph=True to return a dict\n",
    "generate_answer = LangChainPredict(prompt, llm, state_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decide_to_use_retrieve(state):\n",
    "\n",
    "    use_retrieve = False\n",
    "    if state[\"cached_questions\"] is None:\n",
    "        return \"retrieve\"\n",
    "    if state[\"question\"] not in state[\"cached_questions\"]:\n",
    "        use_retrieve = True\n",
    "\n",
    "    if use_retrieve:\n",
    "        print(\"Using retrieve\")\n",
    "        return \"retrieve\"\n",
    "    else:\n",
    "        print(\"Not using retrieve\")\n",
    "        return \"generate_answer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict\n",
    "\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context: str\n",
    "    output: str\n",
    "    cached_questions: list\n",
    "    langpredict_output: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph\n",
    "\n",
    "g = StateGraph(State)\n",
    "\n",
    "g.add_node(\"start\", RunnablePassthrough())\n",
    "g.add_conditional_edges(\"start\", decide_to_use_retrieve)\n",
    "\n",
    "g.add_node(\"retrieve\", RunnablePassthrough.assign(context=retrieve))\n",
    "g.add_edge(\"retrieve\", \"generate_answer\")\n",
    "\n",
    "g.add_node(\"generate_answer\", generate_answer)\n",
    "g.add_edge(\"generate_answer\", END)\n",
    "\n",
    "g.set_entry_point(\"start\")\n",
    "\n",
    "compiled_g = g.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compiled_graph.invoke(\n",
    "#     {\n",
    "#         \"question\": \"what's the capital of Korea\",\n",
    "#         \"cached_questions\": [\n",
    "#             \"what's the capital of France\",\n",
    "#             \"what's the capital of Germany\",\n",
    "#         ],\n",
    "#     }\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Converting the chain into a **DSPy module**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From DSPy, import the modules that know how to interact with LangChain LCEL.\n",
    "from dspy.predict.langchain import LangChainModule\n",
    "\n",
    "graph_dspy_module = LangChainModule(compiled_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pydantic.v1.main.LangGraphInput'>\n",
      "<class 'pydantic.v1.main.LangGraphOutput'>\n",
      "\n",
      "mapper={\n",
      "  context: RunnableLambda(lambda x: dspy.Retrieve(k=5)(x['question']).passages)\n",
      "}\n",
      "LangChainPredict(Template(Essential Instructions: Respond to the given question based on the provided context in the style of a tweet. The response should be concise, engaging, and limited to the character count typical for a tweet (up to 280 characters)., ['Context:', 'Question:', 'Tweet Response:']))\n"
     ]
    }
   ],
   "source": [
    "for name, node in compiled_graph.get_graph().nodes.items():\n",
    "    print(f\"{node.data}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LangChainPredict(Template(Essential Instructions: Respond to the given question based on the provided context in the style of a tweet. The response should be concise, engaging, and limited to the character count typical for a tweet (up to 280 characters)., ['Context:', 'Question:', 'Tweet Response:']))]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_dspy_module.modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph_dspy_module.invoke(\n",
    "#     {\n",
    "#         \"question\": \"what's the capital of Korea\",\n",
    "#         # \"cached_questions\": [\n",
    "#         #     \"what's the capital of France\",\n",
    "#         #     \"what's the capital of Germany\",\n",
    "#         # ],\n",
    "#     }\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Trying the module\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question = \"In what region was Eddy Mazzoleni born?\"\n",
    "\n",
    "# graph_dspy_module.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Optimizing the module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/minkijung/anaconda3/envs/survey_buddy/lib/python3.12/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by promote_options='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(200, 50, 150)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We took the liberty to define this metric and load a few examples from a standard QA dataset.\n",
    "# Let's impore them from `tweet_metric.py` in the same directory that contains this notebook.\n",
    "from tweet_metric import metric, trainset, valset, devset\n",
    "\n",
    "# We loaded 200, 50, and 150 examples for training, validation (tuning), and development (evaluation), respectively.\n",
    "# You could load less (or more) and, chances are, the right DSPy optimizers will work well for many problems.\n",
    "len(trainset), len(valset), len(devset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = trainset[:1]\n",
    "valset = valset[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = BootstrapFewShotWithRandomSearch(metric=metric, max_bootstrapped_demos=1, num_candidate_programs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChainModule forward kwargs =\n",
      " {'question': 'Are both Cangzhou and Qionghai in the Hebei province of China?'}\n",
      "LangGraph Input:  {'question': 'Are both Cangzhou and Qionghai in the Hebei province of China?'}\n",
      "RunnableCallable.invoke INPUT:  {'question': 'Are both Cangzhou and Qionghai in the Hebei province of China?'}\n",
      "RunnableCallable.invoke INPUT:  <object object at 0x1272ee5d0>\n",
      "RunnableCallable.invoke input:  <object object at 0x1272ee5d0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:dspy.evaluate.evaluate:\u001b[2m2024-05-14T20:41:30.621738Z\u001b[0m [\u001b[31m\u001b[1merror    \u001b[0m] \u001b[1mError for example in dev set: \t\t Expected dict, got <object object at 0x1272ee5d0>\u001b[0m [\u001b[0m\u001b[1m\u001b[34mdspy.evaluate.evaluate\u001b[0m]\u001b[0m \u001b[36mfilename\u001b[0m=\u001b[35mevaluate.py\u001b[0m \u001b[36mlineno\u001b[0m=\u001b[35m147\u001b[0m\n",
      "Average Metric: 0.0 / 1  (0.0): 100%|██████████| 1/1 [00:00<00:00, 1293.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChainModule forward kwargs =\n",
      " {'question': 'Are both Cangzhou and Qionghai in the Hebei province of China?'}\n",
      "LangGraph Input:  {'question': 'Are both Cangzhou and Qionghai in the Hebei province of China?'}\n",
      "RunnableCallable.invoke INPUT:  {'question': 'Are both Cangzhou and Qionghai in the Hebei province of China?'}\n",
      "RunnableCallable.invoke INPUT:  <object object at 0x1272ee5b0>\n",
      "RunnableCallable.invoke input:  <object object at 0x1272ee5b0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:dspy.evaluate.evaluate:\u001b[2m2024-05-14T20:41:30.650805Z\u001b[0m [\u001b[31m\u001b[1merror    \u001b[0m] \u001b[1mError for example in dev set: \t\t Expected dict, got <object object at 0x1272ee5b0>\u001b[0m [\u001b[0m\u001b[1m\u001b[34mdspy.evaluate.evaluate\u001b[0m]\u001b[0m \u001b[36mfilename\u001b[0m=\u001b[35mevaluate.py\u001b[0m \u001b[36mlineno\u001b[0m=\u001b[35m147\u001b[0m\n",
      "Average Metric: 0.0 / 1  (0.0): 100%|██████████| 1/1 [00:00<00:00, 1153.23it/s]\n",
      "Average Metric: 0.0 / 4  (0.0):  40%|████      | 4/10 [00:46<01:09, 11.63s/it]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Template' object has no attribute 'equals'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Now use the optimizer to *compile* the chain. This could take 5-10 minutes, unless it's cached.\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m optimized_chain \u001b[38;5;241m=\u001b[39m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph_dspy_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrainset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalset\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/survey_buddy/lib/python3.12/site-packages/dspy/teleprompt/random_search.py:96\u001b[0m, in \u001b[0;36mBootstrapFewShotWithRandomSearch.compile\u001b[0;34m(self, student, teacher, trainset, valset, restrict, labeled_sample)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m seed \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;66;03m# unshuffled few-shot\u001b[39;00m\n\u001b[1;32m     88\u001b[0m     program \u001b[38;5;241m=\u001b[39m BootstrapFewShot(\n\u001b[1;32m     89\u001b[0m         metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetric,\n\u001b[1;32m     90\u001b[0m         metric_threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetric_threshold,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     94\u001b[0m         max_rounds\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_rounds,\n\u001b[1;32m     95\u001b[0m     )\n\u001b[0;32m---> 96\u001b[0m     program2 \u001b[38;5;241m=\u001b[39m \u001b[43mprogram\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mteacher\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mteacher\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrainset2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m seed \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, seed\n",
      "File \u001b[0;32m~/anaconda3/envs/survey_buddy/lib/python3.12/site-packages/dspy/teleprompt/bootstrap.py:60\u001b[0m, in \u001b[0;36mBootstrapFewShot.compile\u001b[0;34m(self, student, teacher, trainset, valset)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalset \u001b[38;5;241m=\u001b[39m valset\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_student_and_teacher(student, teacher)\n\u001b[0;32m---> 60\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_predictor_mappings\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bootstrap()\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstudent \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train()\n",
      "File \u001b[0;32m~/anaconda3/envs/survey_buddy/lib/python3.12/site-packages/dspy/teleprompt/bootstrap.py:92\u001b[0m, in \u001b[0;36mBootstrapFewShot._prepare_predictor_mappings\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (name1, predictor1), (name2, predictor2) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(student\u001b[38;5;241m.\u001b[39mnamed_predictors(), teacher\u001b[38;5;241m.\u001b[39mnamed_predictors()):\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m name1 \u001b[38;5;241m==\u001b[39m name2, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStudent and teacher must have the same program structure.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 92\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[43mpredictor1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mequals\u001b[49m(\n\u001b[1;32m     93\u001b[0m         predictor2\u001b[38;5;241m.\u001b[39msignature,\n\u001b[1;32m     94\u001b[0m     ), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStudent and teacher must have the same signatures. \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(predictor1\u001b[38;5;241m.\u001b[39msignature)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m != \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(predictor2\u001b[38;5;241m.\u001b[39msignature)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mid\u001b[39m(predictor1) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mid\u001b[39m(predictor2), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStudent and teacher must be different objects.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     97\u001b[0m     name2predictor[name1] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# dict(student=predictor1, teacher=predictor2)\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Template' object has no attribute 'equals'"
     ]
    }
   ],
   "source": [
    "# Now use the optimizer to *compile* the chain. This could take 5-10 minutes, unless it's cached.\n",
    "optimized_chain = optimizer.compile(graph_dspy_module, trainset=trainset, valset=valset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "kw = {\n",
    "    \"question\": \"Are both Chico Municipal Airport and William R. Fairchild International Airport in California?\"\n",
    "}\n",
    "\n",
    "print(type(dict(**kw)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AnswerQuestion'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field, ValidationError\n",
    "from langchain_core.messages import HumanMessage, ToolMessage\n",
    "from langchain_core.output_parsers.openai_tools import PydanticToolsParser\n",
    "\n",
    "\n",
    "class Reflection(BaseModel):\n",
    "    missing: str = Field(description=\"Critique of what is missing.\")\n",
    "    superfluous: str = Field(description=\"Critique of what is superfluous\")\n",
    "\n",
    "\n",
    "class AnswerQuestion(BaseModel):\n",
    "    \"\"\"Answer the question. Provide an answer, reflection, and then follow up with search queries to improve the answer.\"\"\"\n",
    "\n",
    "    answer: str = Field(description=\"~250 word detailed answer to the question.\")\n",
    "    reflection: Reflection = Field(description=\"Your reflection on the initial answer.\")\n",
    "    search_queries: list[str] = Field(\n",
    "        description=\"1-3 search queries for researching improvements to address the critique of your current answer.\"\n",
    "    )\n",
    "\n",
    "AnswerQuestion.__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field, validator\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "model = OpenAI(model_name=\"gpt-3.5-turbo-instruct\", temperature=0.0)\n",
    "\n",
    "\n",
    "# Define your desired data structure.\n",
    "class Joke(BaseModel):\n",
    "    setup: str = Field(description=\"question to set up a joke\")\n",
    "    punchline: str = Field(description=\"answer to resolve the joke\")\n",
    "\n",
    "    # You can add custom validation logic easily with Pydantic.\n",
    "    @validator(\"setup\")\n",
    "    def question_ends_with_question_mark(cls, field):\n",
    "        if field[-1] != \"?\":\n",
    "            raise ValueError(\"Badly formed question!\")\n",
    "        return field\n",
    "\n",
    "\n",
    "# Set up a parser + inject instructions into the prompt template.\n",
    "parser = PydanticOutputParser(pydantic_object=Joke)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The output should be formatted as a JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output schema:\\n```\\n{\"properties\": {\"setup\": {\"title\": \"Setup\", \"description\": \"question to set up a joke\", \"type\": \"string\"}, \"punchline\": {\"title\": \"Punchline\", \"description\": \"answer to resolve the joke\", \"type\": \"string\"}}, \"required\": [\"setup\", \"punchline\"]}\\n```'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser.get_format_instructions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# And a query intended to prompt a language model to populate the data structure.\n",
    "prompt_and_model = prompt | model\n",
    "output = prompt_and_model.invoke({\"query\": \"Tell me a joke.\"})\n",
    "parser.invoke(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39_dec_2023",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
