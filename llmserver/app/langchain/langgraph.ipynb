{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langgraph.graph import END, MessageGraph\n",
    "\n",
    "model = ChatOpenAI(temperature=0)\n",
    "\n",
    "graph = MessageGraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = {\n",
    "    \"title\": \"Chatbot\",\n",
    "    \"description\": \"This is a chatbot that can chat with you.\",\n",
    "    \"enough\": 0.8,\n",
    "    \"ref_chat\": [\"a\", \"b\", \"c\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_relevant_question(messages: list):\n",
    "    print(\"messages\", messages)\n",
    "    if document.get(\"enough\") > 0.7:\n",
    "        return AIMessage(\"Yes, I can help you.\")\n",
    "    else:\n",
    "        return AIMessage(\"I'm sorry, I can't help you.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.add_node(\"oracle\", model)\n",
    "graph.add_edge(\"oracle\", \"rel_question\")\n",
    "graph.add_node(\"rel_question\", check_relevant_question)\n",
    "graph.add_edge(\"rel_question\", END)\n",
    "\n",
    "graph.set_entry_point(\"oracle\")\n",
    "\n",
    "runnable = graph.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages [HumanMessage(content='Hello', id='be8180e2-5b56-429b-a3ab-e006e52849c9'), AIMessage(content='Hello! How can I assist you today?', response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 8, 'total_tokens': 17}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-a33fa327-c72a-46c7-a69d-54eb8934f54e-0')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Hello', id='be8180e2-5b56-429b-a3ab-e006e52849c9'),\n",
       " AIMessage(content='Hello! How can I assist you today?', response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 8, 'total_tokens': 17}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-a33fa327-c72a-46c7-a69d-54eb8934f54e-0'),\n",
       " AIMessage(content='Yes, I can help you.', id='788137d9-ccf6-4af1-92b9-6af16b6d6aba')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runnable.invoke(HumanMessage(\"Hello\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "\n",
    "prompt = hub.pull(\"hwchase17/openai-tools-agent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.prompts.chat.ChatPromptTemplate'>\n"
     ]
    }
   ],
   "source": [
    "print(type(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are a helpful assistant'), HumanMessage(content='hello'), HumanMessage(content='nothign much')])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.invoke({\"input\": \"hello\", \"agent_scratchpad\": [\"nothign much\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph2= MessageGraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "def find(messages: list):\n",
    "    print(\"messages\", messages)\n",
    "    return AIMessage(\"I found it.\")\n",
    "\n",
    "passthrough = RunnablePassthrough()\n",
    "\n",
    "graph2.add_node(\"receive\", passthrough)\n",
    "graph2.add_node(\"find\", find)\n",
    "\n",
    "graph2.add_edge(\"receive\", \"find\")\n",
    "graph2.add_edge(\"find\", END)\n",
    "\n",
    "graph2.set_entry_point(\"receive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages [HumanMessage(content='hi', id='0e679f6c-c5f7-4e19-b17d-2dc59fcfcb19'), HumanMessage(content='hello', id='58abe5c3-2e05-4dc8-bfde-1fae06800397')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='hi', id='0e679f6c-c5f7-4e19-b17d-2dc59fcfcb19'),\n",
       " HumanMessage(content='hello', id='58abe5c3-2e05-4dc8-bfde-1fae06800397'),\n",
       " AIMessage(content='I found it.', id='bb87b5e5-4831-49e2-928c-c2200326d9c8')]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph2.compile().invoke([\"hi\", \"hello\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, MessageGraph, StateGraph\n",
    "from typing import TypedDict, Annotated\n",
    "from bson import ObjectId\n",
    "\n",
    "\n",
    "def add_messages(left: list, right: list):\n",
    "    \"\"\"Add instead of overwriting\"\"\"\n",
    "    return left + right\n",
    "\n",
    "\n",
    "def add_ephemeral(old: dict, new: dict):\n",
    "    \"\"\"Add instead of overwriting\"\"\"\n",
    "    return {**old, **new}\n",
    "\n",
    "\n",
    "class DocumentState(TypedDict):\n",
    "    _id: ObjectId\n",
    "    phone_number: str\n",
    "    name: str\n",
    "    created_at: str\n",
    "    updated_at: str\n",
    "    user_info: object\n",
    "    messages: Annotated[list, add_messages]\n",
    "    questions: list\n",
    "    ephemeral: Annotated[dict, add_ephemeral]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0, streaming=True)\n",
    "\n",
    "# def print_name(state: DocumentState):\n",
    "#     print(state[\"name\"])\n",
    "#     response = model.invoke([HumanMessage(\"Hello I am {}\".format(state[\"name\"]))])\n",
    "#     return {\"messages\": [response]}\n",
    "\n",
    "# graph.add_node(\"print_name\", print_name)\n",
    "# graph.add_edge(\"print_name\", END)\n",
    "\n",
    "# graph.set_entry_point(\"print_name\")\n",
    "# graph.compile().invoke({\"_id\": ObjectId(), \"name\": \"John Doe\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "def pick_a_new_question(documentState: DocumentState):\n",
    "    output_parser = StrOutputParser()\n",
    "\n",
    "    unasked_questions = []\n",
    "    for question in documentState[\"questions\"]:\n",
    "        if question[\"enough\"] < documentState[\"ephemeral\"][\"enoughness_threshold\"]:\n",
    "            unasked_questions.append(question[\"content\"])\n",
    "\n",
    "    recent_messages = \" / \".join(documentState[\"messages\"][-4:])\n",
    "    options = \" / \".join(unasked_questions)\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                \"you are a survey bot picking the next question based on the current conversation flow. You'll be provided with the last 4 messages and the list of unasked questions. Choose the next question.\",\n",
    "            ),\n",
    "            (\"human\", \"recent_messages: {recent_messages} \\n options: {options}\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    chain = prompt | model | output_parser\n",
    "    result = chain.invoke({\"recent_messages\": recent_messages, \"options\": options})\n",
    "\n",
    "    return {\"ephemeral\": {\"next_question\": result}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/minkijung/anaconda3/envs/survey_buddy/lib/python3.12/site-packages/pydantic/v1/main.py:998: RuntimeWarning: fields may not start with an underscore, ignoring \"_id\"\n",
      "  warnings.warn(f'fields may not start with an underscore, ignoring \"{f_name}\"', RuntimeWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'_id': ObjectId('663c0b4e5ce313e2caf995c3'),\n",
       " 'name': 'John Doe',\n",
       " 'messages': ['Hello', 'Hi', 'How are you?', 'I am good.'],\n",
       " 'questions': [{'content': 'What is your name?', 'enough': 0.9},\n",
       "  {'content': 'How are you?', 'enough': 0.8},\n",
       "  {'content': 'What is your favorite color?', 'enough': 0.7}],\n",
       " 'ephemeral': {'key': 1,\n",
       "  'key2': 2,\n",
       "  'next_question': 'What is your favorite color?',\n",
       "  'enoughness_threshold': 0.8}}"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph = StateGraph(DocumentState)\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0, streaming=True)\n",
    "\n",
    "graph.add_node(\"pick_a_new_question\", pick_a_new_question)\n",
    "graph.add_edge(\"pick_a_new_question\", END)\n",
    "\n",
    "graph.set_entry_point(\"pick_a_new_question\")\n",
    "\n",
    "graph.compile().invoke(\n",
    "    {\n",
    "        \"_id\": ObjectId(),\n",
    "        \"name\": \"John Doe\",\n",
    "        \"messages\": [\"Hello\", \"Hi\", \"How are you?\", \"I am good.\"],\n",
    "        \"questions\": [\n",
    "            {\"content\": \"What is your name?\", \"enough\": 0.9},\n",
    "            {\"content\": \"How are you?\", \"enough\": 0.8},\n",
    "            {\"content\": \"What is your favorite color?\", \"enough\": 0.7},\n",
    "        ],\n",
    "        \"ephemeral\": {\n",
    "            \"key\": 1,\n",
    "            \"key2\": 2,\n",
    "            \"next_question\": [\"What is your name?\"],\n",
    "            \"enoughness_threshold\": 0.8,\n",
    "        },\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conditional graph\n",
    "\n",
    "msg = HumanMessage(\"Hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lc': 1,\n",
       " 'type': 'constructor',\n",
       " 'id': ['langchain', 'schema', 'messages', 'HumanMessage'],\n",
       " 'kwargs': {'content': 'Hello', 'type': 'human'}}"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msg.to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hi John'"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"hi {input}\"\"\"\n",
    ")\n",
    "prompt.format(input=\"John\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringPromptValue(text='hi <bound method Kernel.raw_input of <ipykernel.ipkernel.IPythonKernel object at 0x103d28c80>>')"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.invoke({\"input\": \"minki\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "survey_buddy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
